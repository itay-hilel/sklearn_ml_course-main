{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dataset ames_housing_no_missing.csv with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ames_housing = pd.read_csv(\"../datasets/ames_housing_no_missing.csv\")\n",
    "target_name = \"SalePrice\"\n",
    "data = ames_housing.drop(columns=target_name)\n",
    "target = ames_housing[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ames_housing is a pandas dataframe. The column \"SalePrice\" contains the target variable.\n",
    "\n",
    "To simplify this exercise, we will only used the numerical features defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\",\n",
    "    \"GrLivArea\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\", \"Fireplaces\",\n",
    "    \"GarageCars\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\",\n",
    "    \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\",\n",
    "]\n",
    "\n",
    "data_numerical = data[numerical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the generalization performance of a decision tree and a linear regression. For this purpose, we will create two separate predictive models and evaluate them by 10-fold cross-validation.\n",
    "\n",
    "Thus, use sklearn.linear_model.LinearRegression and sklearn.tree.DecisionTreeRegressor to create the models. Use the default parameters for both models.\n",
    "\n",
    "Be aware that a linear model requires to scale numerical features. Please use sklearn.preprocessing.StandardScaler so that your linear regression model behaves the same way as the quiz author intended ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "linear_model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "cv_results = cross_validate(linear_model,\n",
    "                            data_numerical, target,\n",
    "                            return_estimator=True, cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76129977, 0.80635105, 0.81358636, 0.66592199, 0.79964891,\n",
       "       0.76868787, 0.75635094, 0.71822127, 0.31479306, 0.78635221])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model mean accuracy: 0.72 +/- 0.14\n"
     ]
    }
   ],
   "source": [
    "print(f'Linear regression model mean accuracy: {cv_results[\"test_score\"].mean():.2f} +/- {cv_results[\"test_score\"].std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_model = DecisionTreeRegressor()\n",
    "\n",
    "tree_cv_results = cross_validate(tree_model,\n",
    "                            data_numerical, target,\n",
    "                            cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57255068, 0.68686058, 0.69413218, 0.57931539, 0.73917407,\n",
       "       0.63367046, 0.54579584, 0.67169015, 0.64038312, 0.6417573 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree regression model mean accuracy: 0.64 +/- 0.06\n"
     ]
    }
   ],
   "source": [
    "print(f'Decision tree regression model mean accuracy: {tree_cv_results[\"test_score\"].mean():.2f} +/- {tree_cv_results[\"test_score\"].std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression is better than decision tree for 9 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Linear regression is better than decision tree for '\n",
    "    f'{sum(cv_results[\"test_score\"] > tree_cv_results[\"test_score\"])} CV iterations out of 10 folds.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the default parameters for the decision tree regressor, we will optimize the max_depth of the tree. Vary the max_depth from 1 level up to 15 levels. Use nested cross-validation to evaluate a grid-search (sklearn.model_selection.GridSearchCV). Set cv=10 for both the inner and outer cross-validations, then answer the questions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "max_depth = np.arange(1, 16, 1)\n",
    "param_grid = {'max_depth': max_depth}\n",
    "\n",
    "inner_cv = GridSearchCV(DecisionTreeRegressor(),\n",
    "                        param_grid=param_grid,\n",
    "                        cv=10)\n",
    "\n",
    "gs_cv_results = cross_validate(inner_cv,\n",
    "                               data_numerical, target,\n",
    "                               return_estimator=True,\n",
    "                               cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6}\n",
      "{'max_depth': 5}\n",
      "{'max_depth': 6}\n",
      "{'max_depth': 5}\n",
      "{'max_depth': 6}\n",
      "{'max_depth': 6}\n",
      "{'max_depth': 5}\n",
      "{'max_depth': 6}\n",
      "{'max_depth': 8}\n",
      "{'max_depth': 13}\n"
     ]
    }
   ],
   "source": [
    "for search_cv in gs_cv_results['estimator']:\n",
    "    print(search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time max_depth was in the range 5 to 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to evaluate the generalization performance of the decision tree while taking into account the fact that we tune the depth for this specific dataset. Use the grid-search as an estimator inside a cross_validate to automatically tune the max_depth parameter on each cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69038582, 0.76720246, 0.71227376, 0.4745161 , 0.76417411,\n",
       "       0.73820259, 0.66928018, 0.7666175 , 0.47670107, 0.69585703])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv_results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree with optimized via GridSearchCV max_depth parameter regression model mean accuracy: 0.68 +/- 0.11\n"
     ]
    }
   ],
   "source": [
    "print(f'Decision tree with optimized via GridSearchCV max_depth parameter regression model mean accuracy: {gs_cv_results[\"test_score\"].mean():.2f} +/- {gs_cv_results[\"test_score\"].std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression is better than decision tree for 8 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Linear regression is better than decision tree for '\n",
    "    f'{sum(cv_results[\"test_score\"] > gs_cv_results[\"test_score\"])} CV iterations out of 10 folds.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using only the numerical features you will now use the entire dataset available in the variable data.\n",
    "\n",
    "Create a preprocessor by dealing separately with the numerical and categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_processor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (categorical_processor, selector(dtype_include=object)),\n",
    "    (\"passthrough\", selector(dtype_exclude=object))\n",
    ")\n",
    "tree = make_pipeline(preprocessor,\n",
    "                     DecisionTreeRegressor(max_depth=7, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    tree, data, target, cv=10, return_estimator=True, n_jobs=2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72939283, 0.78302269, 0.82629515, 0.74948593, 0.8330028 ,\n",
       "       0.85093205, 0.78903061, 0.75170173, 0.60072763, 0.75856634])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree regression model with both categorical and numerical features mean accuracy: 0.77 +/- 0.07\n"
     ]
    }
   ],
   "source": [
    "print(f'Decision tree regression model with both categorical and numerical features mean accuracy: {cv_results[\"test_score\"].mean():.2f} +/- {cv_results[\"test_score\"].std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tree model using both numerical and categorical features is better than a tree with optimal depth using only numerical features for 9 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'A tree model using both numerical and categorical features is better than a '\n",
    "    'tree with optimal depth using only numerical features for '\n",
    "    f'{sum(cv_results[\"test_score\"] > gs_cv_results[\"test_score\"])} CV '\n",
    "    'iterations out of 10 folds.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
