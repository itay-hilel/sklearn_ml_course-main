{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3c56fa",
   "metadata": {},
   "source": [
    "# Introductory example to ensemble models\n",
    "\n",
    "This first notebook aims at emphasizing the benefit of ensemble methods over\n",
    "simple models (e.g. decision tree, linear model, etc.). Combining simple\n",
    "models result in more powerful and robust models with less hassle.\n",
    "\n",
    "We will start by loading the california housing dataset. We recall that the\n",
    "goal in this dataset is to predict the median house value in some district\n",
    "in California based on demographic and geographic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d28208",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">If you want a deeper overview regarding this dataset, you can refer to the\n",
    "Appendix - Datasets description section at the end of this MOOC.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7abc1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5006aa1",
   "metadata": {},
   "source": [
    "We will check the generalization performance of decision tree regressor with\n",
    "default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3ac567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.354 +/- 0.087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "cv_results = cross_validate(tree, data, target, n_jobs=2)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862cf51",
   "metadata": {},
   "source": [
    "We obtain fair results. However, as we previously presented in the \"tree in\n",
    "depth\" notebook, this model needs to be tuned to overcome over- or\n",
    "under-fitting. Indeed, the default parameters will not necessarily lead to an\n",
    "optimal decision tree. Instead of using the default value, we should search\n",
    "via cross-validation the optimal value of the important parameters such as\n",
    "`max_depth`, `min_samples_split`, or `min_samples_leaf`.\n",
    "\n",
    "We recall that we need to tune these parameters, as decision trees tend to\n",
    "overfit the training data if we grow deep trees, but there are no rules on\n",
    "what each parameter should be set to. Thus, not making a search could lead us\n",
    "to have an underfitted or overfitted model.\n",
    "\n",
    "Now, we make a grid-search to tune the hyperparameters that we mentioned\n",
    "earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6275359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.523 +/- 0.107\n",
      "CPU times: user 22.2 ms, sys: 8.54 ms, total: 30.7 ms\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 8, None],\n",
    "    \"min_samples_split\": [2, 10, 30, 50],\n",
    "    \"min_samples_leaf\": [0.01, 0.05, 0.1, 1]}\n",
    "cv = 3\n",
    "\n",
    "tree = GridSearchCV(DecisionTreeRegressor(random_state=0),\n",
    "                    param_grid=param_grid, cv=cv, n_jobs=2)\n",
    "cv_results = cross_validate(tree, data, target, n_jobs=2,\n",
    "                            return_estimator=True)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929cfb6",
   "metadata": {},
   "source": [
    "We see that optimizing the hyperparameters will have a positive effect\n",
    "on the generalization performance. However, it comes with a higher computational\n",
    "cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66932624",
   "metadata": {},
   "source": [
    "We can create a dataframe storing the important information collected during\n",
    "the tuning of the parameters and investigate the results.\n",
    "\n",
    "Now we will use an ensemble method called bagging. More details about this\n",
    "method will be discussed in the next section. In short, this method will use\n",
    "a base regressor (i.e. decision tree regressors) and will train several of\n",
    "them on a slightly modified version of the training set. Then, the\n",
    "predictions of all these base regressors will be combined by averaging.\n",
    "\n",
    "Here, we will use 20 decision trees and check the fitting time as well as the\n",
    "generalization performance on the left-out testing data. It is important to note\n",
    "that we are not going to tune any parameter of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f70ade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.642 +/- 0.083\n",
      "CPU times: user 39.8 ms, sys: 9.89 ms, total: 49.7 ms\n",
      "Wall time: 7.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_estimator = DecisionTreeRegressor(random_state=0)\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    base_estimator=base_estimator, n_estimators=20, random_state=0)\n",
    "\n",
    "cv_results = cross_validate(bagging_regressor, data, target, n_jobs=2)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d87597",
   "metadata": {},
   "source": [
    "Without searching for optimal hyperparameters, the overall generalization\n",
    "performance of the bagging regressor is better than a single decision tree.\n",
    "In addition, the computational cost is reduced in comparison of seeking\n",
    "for the optimal hyperparameters.\n",
    "\n",
    "This shows the motivation behind the use of an ensemble learner: it gives a\n",
    "relatively good baseline with decent generalization performance without any\n",
    "parameter tuning.\n",
    "\n",
    "Now, we will discuss in detail two ensemble families: bagging and\n",
    "boosting:\n",
    "\n",
    "* ensemble using bootstrap (e.g. bagging and random-forest);\n",
    "* ensemble using boosting (e.g. adaptive boosting and gradient-boosting\n",
    "  decision tree)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nbreset": "https://github.com/INRIA/scikit-learn-mooc/raw/main/notebooks/ensemble_introduction.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
